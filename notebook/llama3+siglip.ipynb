{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_lwLs3JZvwR"
      },
      "source": [
        "# langauge+vision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0J5lbxNjHWO"
      },
      "source": [
        "## import library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7wFkgx6UoFf",
        "outputId": "c757449c-f7d8-460f-e3d6-07ffddec0e90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2024.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m124.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m210.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip install datasets\n",
        "! pip install peft bitsandbytes accelerate\n",
        "! pip install trl\n",
        "! pip install lightning\n",
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzfslDq0NA9I"
      },
      "outputs": [],
      "source": [
        "!git clone https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7z3g7lyhNBuh"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "zip_file_path = \"/content/LLaVA-CC3M-Pretrain-595K/images.zip\"\n",
        "# 압축을 풀 디렉토리 경로\n",
        "extract_to =\"/content/LLaVA-CC3M-Pretrain-595K/extracted_images\"\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_L0aw3rIFxt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.set_float32_matmul_precision('medium')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSQYq3-m_41p"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRANid3DJZc8"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    LlamaForCausalLM,\n",
        "    SiglipImageProcessor,\n",
        "    SiglipVisionModel,\n",
        "    AutoProcessor,\n",
        "    TrainingArguments,\n",
        "    LlavaForConditionalGeneration,\n",
        ")\n",
        "from transformers import TextStreamer\n",
        "from peft import get_peft_model, LoraConfig\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from transformers.data.data_collator import DataCollatorForLanguageModeling\n",
        "import lightning as L\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import re\n",
        "from nltk import edit_distance\n",
        "import numpy as np\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from io import BytesIO\n",
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "from trl import SFTTrainer, SFTConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44Ioulzjim7X"
      },
      "outputs": [],
      "source": [
        "config = {\"max_epochs\": 1,\n",
        "          \"val_check_interval\": 0.05, # how many times we want to validate during an epoch\n",
        "          \"check_val_every_n_epoch\": 1,\n",
        "          \"gradient_clip_val\": 1.0,\n",
        "          \"accumulate_grad_batches\": 8,\n",
        "          \"lr\": 1e-5,\n",
        "          \"batch_size\": 2,\n",
        "          # \"seed\":24,\n",
        "          \"num_nodes\": 1,\n",
        "          \"warmup_steps\": 50,\n",
        "          \"result_path\": \"./result\",\n",
        "          \"verbose\": True,\n",
        "          \"except_image_max_length\": 64,\n",
        "          \"model_name\": \"unsloth/llama-3-8b-Instruct\",\n",
        "          \"vision_model_name\": \"google/siglip-so400m-patch14-384\",\n",
        "          \"model_embedding_size\": 4096,\n",
        "          \"vision_model_embedding_size\": 1152,\n",
        "          \"image_size\" : 384\n",
        "}\n",
        "\n",
        "# \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
        "# \"unsloth/llama-3-8b-Instruct\"\n",
        "# \"microsoft/Phi-3-mini-4k-instruct\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XaW07_KJT55"
      },
      "outputs": [],
      "source": [
        "# !git clone https://huggingface.co/qresearch/llama-3-vision-alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqcpNc2S9nnY"
      },
      "source": [
        "## model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7ZWVAwzJZiC"
      },
      "outputs": [],
      "source": [
        "def initialize_models():\n",
        "\n",
        "    llm = config.get(\"model_name\")\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_type=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        llm, use_fast=True\n",
        "    )\n",
        "    tokenizer.padding_side = \"right\"\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        llm,\n",
        "        # torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config,\n",
        "        attn_implementation=\"eager\",\n",
        "        output_hidden_states = True,\n",
        "        return_dict_in_generate = True,\n",
        "    )\n",
        "\n",
        "    for param in model.base_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    model_name = config.get(\"vision_model_name\")\n",
        "    # model_name = \"\"\n",
        "    vision_model = SiglipVisionModel.from_pretrained(\n",
        "        model_name,\n",
        "        # torch_dtype=torch.float16\n",
        "    )\n",
        "    processor = SiglipImageProcessor.from_pretrained(model_name)\n",
        "    vision_model = vision_model.to(\"cuda\")\n",
        "\n",
        "    return tokenizer, model, vision_model, processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqnOGVS1JZkg"
      },
      "outputs": [],
      "source": [
        "class ProjectionModule(nn.Module):\n",
        "    def __init__(self, mm_hidden_size, hidden_size):\n",
        "        super(ProjectionModule, self).__init__()\n",
        "\n",
        "        # Directly set up the sequential model\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(mm_hidden_size, hidden_size),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fn0eH-_JkK4"
      },
      "outputs": [],
      "source": [
        "# def load_projection_module(mm_hidden_size=1152, hidden_size=4096, device=\"cuda\"):\n",
        "#     projection_module = ProjectionModule(mm_hidden_size, hidden_size)\n",
        "#     checkpoint = torch.load(\"./mm_projector.bin\")\n",
        "#     # checkpoint = state_dict\n",
        "#     checkpoint = {k.replace(\"mm_projector.\", \"\"): v for k, v in checkpoint.items()}\n",
        "#     projection_module.load_state_dict(checkpoint)\n",
        "#     projection_module = projection_module.to(device).half()\n",
        "#     return projection_module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyzvAi_FagVI"
      },
      "outputs": [],
      "source": [
        "tokenizer, model, vision_model, processor = initialize_models()\n",
        "tokenizer.eos_token = \"<|eot_id|>\"\n",
        "projection_module = ProjectionModule(mm_hidden_size=config.get(\"vision_model_embedding_size\"), hidden_size=config.get(\"model_embedding_size\")) #4096, 3072\n",
        "projection_module = projection_module.to(\"cuda\") #.half()\n",
        "# state_dict = new_dict()\n",
        "# projection_module = load_projection_module()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lb_43UOH4DGr"
      },
      "source": [
        "## dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thmjyGiBCmOo"
      },
      "outputs": [],
      "source": [
        "def tokenizer_image_token(prompt, tokenizer,max_length=config.get(\"except_image_max_length\"),  image_token_index= 500000):\n",
        "\n",
        "    prompt_chunks = prompt.split(\"<image>\")\n",
        "    tokenized_chunks = [tokenizer(chunk, truncation = True, padding = True,max_length=max_length).input_ids for chunk in prompt_chunks]\n",
        "    input_ids = tokenized_chunks[0]\n",
        "\n",
        "    for chunk in tokenized_chunks[1:]:\n",
        "        input_ids.append(image_token_index)\n",
        "        input_ids.extend(chunk[1:])  # Exclude BOS token on nonzero index\n",
        "\n",
        "    attention_mask = torch.ones(len(input_ids), dtype=torch.long)\n",
        "\n",
        "    return torch.tensor(input_ids, dtype=torch.long), attention_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtGWtHJdAB0D"
      },
      "outputs": [],
      "source": [
        "LLAVA_CHAT_TEMPLATE = \"\"\"{% for message in messages %} \\\n",
        "  {% if message['from'] == 'human' %}\n",
        "    USER: {{ message['value'] }} \\\n",
        "  {% else %}\n",
        "    ASSISTANT: {{ message['value'] }} \\\n",
        "  {% endif %} \\\n",
        "  {% if message['from'] == 'gpt' %} \\\n",
        "  {% else %} \\\n",
        "      {{ eos_token }} \\\n",
        "  {% endif %} \\\n",
        "{% endfor %}\"\"\"\n",
        "\n",
        "tokenizer.chat_template = LLAVA_CHAT_TEMPLATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ok-CesuQ4ENL"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self,dataset,tokenizer, processor, train = True , max_length=512, image_size = 256):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.processor = processor\n",
        "        self.max_length = max_length\n",
        "        self.train = train\n",
        "        self.dataset = dataset\n",
        "        self.image_size = image_size\n",
        "        self.dataset_length = len(self.dataset)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.dataset_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch = self.dataset[idx]\n",
        "        texts = batch['conversations']\n",
        "        text = self.tokenizer.apply_chat_template(texts,tokenize=False)\n",
        "        input_id = tokenizer_image_token(text, self.tokenizer, max_length=self.max_length)[0].unsqueeze(0)\n",
        "        attention_mask = tokenizer_image_token(text, self.tokenizer, max_length=self.max_length)[1].unsqueeze(0)\n",
        "\n",
        "        img_name = batch['image']\n",
        "        imgs = os.path.join(\"/content/LLaVA-CC3M-Pretrain-595K/extracted_images/\", img_name)\n",
        "        imgs = Image.open(imgs)\n",
        "        image = imgs.convert(\"RGB\")\n",
        "        image_inputs = self.processor(\n",
        "            images=image, # [image],\n",
        "            return_tensors=\"pt\",\n",
        "            do_resize=True,\n",
        "            size={\"height\": self.image_size, \"width\": self.image_size},\n",
        "        )\n",
        "        pixel_values = image_inputs[\"pixel_values\"]\n",
        "\n",
        "        if self.train:\n",
        "            result = {'input_ids': input_id, 'attention_mask' : attention_mask ,'pixel_values': pixel_values , 'texts': text}\n",
        "        else:\n",
        "            question = text.split(\"<|eot_id|>\")[0]\n",
        "            question += \" ASSISTANT:\"\n",
        "            answer = text.split(\"<|eot_id|>\")[1].replace(\"ASSISTANT:\", \"\")\n",
        "            result = {'input_ids': input_id, 'attention_mask' : attention_mask, 'pixel_values': pixel_values , 'questions': question, 'answers': answer }\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yu74ihvm4EXW"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "# raw_datasets = load_dataset(\"lmms-lab/LLaVA-OneVision-Data\", 'TabMWP(MathV360K)')\n",
        "raw_datasets = load_dataset(\"liuhaotian/LLaVA-CC3M-Pretrain-595K\",data_files = \"chat.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjcvJiKG4EaF"
      },
      "outputs": [],
      "source": [
        "train = raw_datasets[\"train\"]\n",
        "\n",
        "train_valid_split = train.train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
        "\n",
        "# Access the train and validation sets\n",
        "train_split = train_valid_split['train']\n",
        "valid_split = train_valid_split['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXG6Knd84EfQ"
      },
      "outputs": [],
      "source": [
        "train_dataset = CustomDataset(train_split,tokenizer, processor , train = True,max_length=config.get(\"except_image_max_length\"), image_size = config.get(\"image_size\"))\n",
        "val_dataset = CustomDataset(valid_split,tokenizer, processor , train = False,max_length=config.get(\"except_image_max_length\"), image_size = config.get(\"image_size\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AQynicG_Uq-"
      },
      "outputs": [],
      "source": [
        "class DataCollatorForCustomVLM(DataCollatorForLanguageModeling):\n",
        "    def __init__(self, tokenizer, mlm=False, train = True):\n",
        "        super().__init__(tokenizer, mlm)\n",
        "        self.train = train\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        input_ids = [item['input_ids'].squeeze(0) for item in batch]\n",
        "        attention_mask = [item['attention_mask'].squeeze(0) for item in batch]\n",
        "        pixel_values = [item['pixel_values'] for item in batch]\n",
        "\n",
        "\n",
        "        input_ids_padded = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
        "        attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
        "\n",
        "        pixel_values = torch.cat(pixel_values, dim = 0)\n",
        "\n",
        "        labels = input_ids_padded.clone()\n",
        "        if self.tokenizer.pad_token_id is not None:\n",
        "            labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        if self.train:\n",
        "          returns =  {\n",
        "              'input_ids': input_ids_padded,\n",
        "              'attention_mask':attention_mask,\n",
        "              'pixel_values': pixel_values,\n",
        "              'labels': labels,\n",
        "          }\n",
        "        else:\n",
        "          answers = [item['answers'] for item in batch]\n",
        "          returns =  {\n",
        "              'input_ids': input_ids_padded,\n",
        "              'attention_mask':attention_mask,\n",
        "              'pixel_values': pixel_values,\n",
        "              'labels': labels,\n",
        "              'answers': answers\n",
        "          }\n",
        "\n",
        "        return returns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYlCeoZPlvLI"
      },
      "source": [
        " ## custom vision langauge model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_p8lknBAJZfc"
      },
      "outputs": [],
      "source": [
        "def process_tensors(input_ids, attention_mask, image_features, embedding_layer):\n",
        "\n",
        "    total_ids = []\n",
        "    total_attn = []\n",
        "\n",
        "    for i in range(input_ids.shape[0]):\n",
        "\n",
        "      input = input_ids[i].unsqueeze(0)\n",
        "      attention = attention_mask[i].unsqueeze(0)\n",
        "      image = image_features[i].unsqueeze(0)\n",
        "\n",
        "      if not isinstance(input, torch.Tensor):\n",
        "          input = torch.tensor(input)\n",
        "\n",
        "      split_index = (input == 500000).nonzero(as_tuple=True)[1]\n",
        "\n",
        "      input_ids_1 = input[:, :split_index]\n",
        "      input_ids_2 = input[:, split_index + 1 :]\n",
        "\n",
        "      # Convert input_ids to embeddings\n",
        "      embeddings_1 = embedding_layer(input_ids_1)\n",
        "      embeddings_2 = embedding_layer(input_ids_2)\n",
        "\n",
        "      device = image.device\n",
        "      token_embeddings_part1 = embeddings_1.to(device)\n",
        "      token_embeddings_part2 = embeddings_2.to(device)\n",
        "\n",
        "      # Concatenate the token embeddings and image features\n",
        "      concatenated_embedding = torch.cat(\n",
        "          [token_embeddings_part1, image, token_embeddings_part2], dim=1\n",
        "      )\n",
        "\n",
        "      attention_mask_1 = attention[:, :split_index]\n",
        "      attention_mask_2 = attention[:, split_index + 1 :]\n",
        "      image = torch.ones(\n",
        "          image.shape[:2], dtype=torch.long\n",
        "      )\n",
        "      idevice = image.device\n",
        "      cat_attention_mask = torch.cat(\n",
        "          [attention_mask_1.to(idevice), image, attention_mask_2.to(idevice)], dim=1\n",
        "      )\n",
        "\n",
        "      # # Create the corrected attention mask\n",
        "      # attention_mask = torch.ones(\n",
        "      #     concatenated_embedding.shape[:2], dtype=torch.long\n",
        "      # )\n",
        "\n",
        "      total_ids.append(concatenated_embedding)\n",
        "\n",
        "      total_attn.append(cat_attention_mask)\n",
        "\n",
        "    concatenated_embeddings = torch.cat(total_ids, dim=0)\n",
        "    cat_attention_masks = torch.cat(total_attn, dim=0)\n",
        "    return concatenated_embeddings , cat_attention_masks\n",
        "\n",
        "def process_labels(input_ids, image_features):\n",
        "\n",
        "    total_embed = []\n",
        "\n",
        "    for i in range(input_ids.shape[0]):\n",
        "      input = input_ids[i].unsqueeze(0)\n",
        "      image = image_features[i].unsqueeze(0)\n",
        "\n",
        "      if not isinstance(input, torch.Tensor):\n",
        "          input = torch.tensor(input)\n",
        "\n",
        "      split_index = (input == 500000).nonzero(as_tuple=True)[1][0]\n",
        "\n",
        "      input_ids_1 = input[:, :split_index]\n",
        "      input_ids_2 = input[:, split_index + 1 :]\n",
        "\n",
        "      device = image.device\n",
        "      pbatch = image.shape[0]\n",
        "      pseq = image.shape[1]\n",
        "      image_token = torch.full([pbatch,pseq,], -100, dtype=torch.long).to(device)\n",
        "\n",
        "      # Concatenate the token embeddings and image features\n",
        "      concatenated_embedding = torch.cat(\n",
        "          [input_ids_1, image_token, input_ids_2], dim=1\n",
        "      )\n",
        "      total_embed.append(concatenated_embedding)\n",
        "\n",
        "    concatenated_embeddings = torch.cat(total_embed, dim=0)\n",
        "    return concatenated_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8BbToNc3WIE"
      },
      "outputs": [],
      "source": [
        "class custom_vlm(L.LightningModule):\n",
        "    strict_loading = False\n",
        "\n",
        "    def __init__(self, config, vision_model, model, projection_module, tokenizer):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(ignore=[\"vision_model\", \"model\", \"projection_module\", \"tokenizer\"])\n",
        "        self.config = config\n",
        "        self.model = model\n",
        "        self.vision_model = vision_model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.projection_module = projection_module\n",
        "        self.batch_size = config.get(\"batch_size\")\n",
        "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "        self.automatic_optimization = True # 주석 가능\n",
        "\n",
        "    def on_fit_start(self):\n",
        "    # def on_train_start(self):\n",
        "        self.model.eval()\n",
        "        self.vision_model.eval()\n",
        "        self.projection_module.train()\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # self.model.eval()\n",
        "        # self.vision_model.eval()\n",
        "        self.projection_module.train()\n",
        "        # opt = self.optimizers() # automatic_optimization F\n",
        "\n",
        "        input_ids = batch['input_ids'].to(self.device) # value long\n",
        "        pixel_values = batch['pixel_values'].to(self.device)\n",
        "        attention_mask = batch['attention_mask']\n",
        "\n",
        "        # with autocast():\n",
        "        image_forward_outs = self.vision_model(\n",
        "            pixel_values.to(device=self.device,dtype=torch.float16), #.unsqueeze(0),\n",
        "            output_hidden_states=True,\n",
        "        ) # value float16\n",
        "\n",
        "        image_features = image_forward_outs.hidden_states[-2]\n",
        "        projected_embeddings = self.projection_module(image_features).to(self.device) # module float32 + value float16\n",
        "\n",
        "        embedding_layer = self.model.get_input_embeddings()\n",
        "\n",
        "        # 배치 데이터 가능\n",
        "        new_embeds , attn_mask = process_tensors(\n",
        "            input_ids, attention_mask, projected_embeddings, embedding_layer\n",
        "        ) # value float16\n",
        "\n",
        "        labels = process_labels(\n",
        "            input_ids, projected_embeddings\n",
        "        ) # value float16\n",
        "\n",
        "        attn_mask = attn_mask.to(self.device)\n",
        "        new_embeds = new_embeds.to(self.device)\n",
        "        labels = labels.to(self.device)\n",
        "\n",
        "        outputs = self.model(inputs_embeds=new_embeds, attention_mask=attn_mask) # module float32 + value float16\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # logits = logits.view(-1, logits.size(-1))  # Shape: [batch_size * sequence_length, vocab_size]\n",
        "        # labels = labels.view(-1)  # Shape: [batch_size * sequence_length]\n",
        "\n",
        "        shifted_labels = labels[:, 1:]  # t+1 시점 예측\n",
        "        shifted_logits = logits[:, :-1, :]  # t 시점 출력\n",
        "\n",
        "        loss = self.loss_fn(\n",
        "            shifted_logits.contiguous().view(-1, shifted_logits.size(-1)),\n",
        "            shifted_labels.contiguous().view(-1)\n",
        "        )\n",
        "\n",
        "        # opt.zero_grad() # automatic_optimization F\n",
        "        # self.manual_backward(loss) # automatic_optimization F\n",
        "        # opt.step() # automatic_optimization F\n",
        "\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=False, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
        "        # self.model.eval()\n",
        "        # self.vision_model.eval()\n",
        "        self.projection_module.eval()\n",
        "\n",
        "\n",
        "        answers = batch['answers']\n",
        "        input_ids = batch['input_ids'].to(self.device) # value long\n",
        "        pixel_values = batch['pixel_values'].to(self.device)\n",
        "        attention_mask = batch['attention_mask']\n",
        "\n",
        "        # answers = batch[0]['answers']\n",
        "        # input_ids = batch[0]['input_ids'] # value long\n",
        "        # pixel_values = batch[0]['pixel_values']\n",
        "\n",
        "        # with autocast():\n",
        "        image_forward_outs = self.vision_model(\n",
        "            pixel_values.to(device=self.device,dtype=torch.float16), #.unsqueeze(0),\n",
        "            output_hidden_states=True,\n",
        "        ) # value float16\n",
        "\n",
        "        image_features = image_forward_outs.hidden_states[-2]\n",
        "        projected_embeddings = self.projection_module(image_features).to(self.device) # module float32 + value float16\n",
        "\n",
        "        embedding_layer = self.model.get_input_embeddings()\n",
        "\n",
        "        # 배치 데이터 가능\n",
        "        new_embeds, attn_mask = process_tensors(\n",
        "            input_ids, attention_mask, projected_embeddings, embedding_layer\n",
        "        ) # value float16\n",
        "\n",
        "        attn_mask = attn_mask.to(self.device)\n",
        "        new_embeds = new_embeds.to(self.device)\n",
        "\n",
        "        # autoregressively generate token IDs\n",
        "        generated_ids = self.model.generate(inputs_embeds=new_embeds.to(dtype=torch.float16), attention_mask=attn_mask, max_new_tokens=128)\n",
        "        # turn them back into text, chopping of the prompt\n",
        "        # important: we don't skip special tokens here, because we want to see them in the output\n",
        "\n",
        "        sequences = generated_ids[\"sequences\"]\n",
        "        predictions = self.tokenizer.batch_decode(sequences[:, input_ids.size(1):], skip_special_tokens=True)\n",
        "\n",
        "        scores = []\n",
        "        for pred, answer in zip(predictions, answers):\n",
        "            pred = re.sub(r\"(?:(?<=>) | (?=</s_))\", \"\", pred)\n",
        "            scores.append(edit_distance(pred, answer) / max(len(pred), len(answer)))\n",
        "\n",
        "            if self.config.get(\"verbose\", False) and len(scores) == 1:\n",
        "                print(f\"Prediction: {pred}\")\n",
        "                print(f\"    Answer: {answer}\")\n",
        "                print(f\" Normed ED: {scores[0]}\")\n",
        "\n",
        "        self.log(\"val_edit_distance\", np.mean(scores), on_step=True, on_epoch=False, prog_bar=True, logger=True)\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # you could also add a learning rate scheduler if you want\n",
        "        optimizer = torch.optim.AdamW(self.projection_module.parameters(), lr=self.config.get(\"lr\"))\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        data_collator = DataCollatorForCustomVLM(tokenizer=tokenizer, mlm=False, train = True)\n",
        "        return DataLoader(train_dataset, collate_fn=data_collator, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        data_collator = DataCollatorForCustomVLM(tokenizer=tokenizer, mlm=False, train = False)\n",
        "        return DataLoader(val_dataset, collate_fn=data_collator, batch_size=self.batch_size, shuffle=False, num_workers=4)\n",
        "    # def val_dataloader(self):\n",
        "    #     return DataLoader(val_dataset, collate_fn=data_collator2, batch_size=1, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpbU31TB9yry"
      },
      "source": [
        "## training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEXshRlmHI9q"
      },
      "outputs": [],
      "source": [
        "print(model.get_input_embeddings())\n",
        "print(vision_model.get_input_embeddings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpxlTAArhIZl"
      },
      "outputs": [],
      "source": [
        "for name, module in projection_module.named_modules():\n",
        "    if isinstance(module,torch.nn.Linear):\n",
        "        module.weight.requires_grad = True\n",
        "        module.bias.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umOZS7AKnT0M"
      },
      "outputs": [],
      "source": [
        "for param in vision_model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxLfN8Phhb0h"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfuC5S48X_mu"
      },
      "outputs": [],
      "source": [
        "# model_module = custom_vlm(config, vision_model, model, projection_module, tokenizer)\n",
        "# ckpt_path = \"/content/drive/MyDrive/model/lightning_logs/version_10/checkpoints/epoch=0-step=11907.ckpt\"\n",
        "\n",
        "model_module = custom_vlm( # .load_from_checkpoint\n",
        "    # ckpt_path,\n",
        "    config=config,\n",
        "    vision_model=vision_model,\n",
        "    model=model,\n",
        "    projection_module=projection_module,\n",
        "    tokenizer=tokenizer,\n",
        "    # strict=False  # 이거 추가!\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2SZ4q-4kPuW"
      },
      "outputs": [],
      "source": [
        "sum(p.numel() for p in projection_module.parameters() if p.requires_grad == True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17qEbgj1HoEL"
      },
      "outputs": [],
      "source": [
        "total_params = sum(p.numel() for p in model_module.parameters() if p.requires_grad == True)\n",
        "print(f\"Total trainable parameters: {total_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEw--bi4AFUC"
      },
      "outputs": [],
      "source": [
        "cd /content/drive/MyDrive/model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_UmmuCJQI_J"
      },
      "outputs": [],
      "source": [
        "from lightning.pytorch.callbacks import ModelCheckpoint\n",
        "from lightning.pytorch.loggers import WandbLogger\n",
        "\n",
        "# wandb_logger = WandbLogger(project=WANDB_PROJECT, name=WANDB_NAME)\n",
        "\n",
        "# 예: 1000 step마다 저장\n",
        "checkpoint_callback = ModelCheckpoint(    # 저장 경로\n",
        "    filename=\"step-{step}\",         # 저장 파일 이름 패턴\n",
        "    monitor = \"val_edit_distance\",\n",
        "    mode = \"min\",\n",
        "    save_top_k=1,\n",
        ")\n",
        "\n",
        "last_callback = ModelCheckpoint(    # 저장 경로\n",
        "    filename=\"latest-step-{step}\",         # 저장 파일 이름 패턴\n",
        "    monitor = \"step\",\n",
        "    mode = \"max\",\n",
        "    every_n_train_steps= 10000,\n",
        "    save_top_k=1,\n",
        ")\n",
        "\n",
        "trainer = L.Trainer(\n",
        "    accelerator=\"gpu\",\n",
        "    devices=[0],\n",
        "    max_epochs=config.get(\"max_epochs\"),\n",
        "    accumulate_grad_batches=config.get(\"accumulate_grad_batches\"),\n",
        "    check_val_every_n_epoch=config.get(\"check_val_every_n_epoch\"),\n",
        "    gradient_clip_val=config.get(\"gradient_clip_val\"),\n",
        "    precision=\"bf16-mixed\",\n",
        "    limit_val_batches=10,\n",
        "    num_sanity_val_steps=0,\n",
        "    val_check_interval=config.get(\"val_check_interval\"),\n",
        "    gradient_clip_algorithm=\"norm\",\n",
        "    # callbacks=[last_callback,checkpoint_callback],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5f2256826f274f8fa687ea43a99bb64b",
            "a13f87d131654ce7bf5f12726179aa20",
            "a44ad2248aee42e0b100ea03d381e8e4",
            "345144ccab914e2d8a62279db9912dcb",
            "7df540fb8887409c88e4625e6d9039ca",
            "ac35f286e18f4116b3f097c8605b2b0d",
            "cf18f986903d470db5027076328a4c4e",
            "a30f96d101aa4f55b2b5df0923638912",
            "99133bade4ed479797e17a63c62aede6",
            "6066e4c201bd4c14ae708a4d1aa03d37",
            "111c0414d7e44060ad69bf50e1139109",
            "f3508455d7564a4fabf31d8f62299b8d",
            "60bf3b0eb2dc4fb08dc6a95571ee488d",
            "4b9bde8de9814c46a50d4ea01c5e3b9a",
            "3549fb9401e54c59a51cd7f7c37ed7df",
            "b350cce1d4cf420fa9d690ce4841b2b6",
            "c1143e07384547e2b0d85d3a28a99a40",
            "f1f89db35ad24514991024b4a6fea069",
            "b34536a60f404b6d9b5106ef3b69ce6d",
            "858824b05c6e47bbbe0209f1dc7ab434",
            "813fc61e4ccc45509f64276dcc023e91",
            "78270360941740f987c942f3139d5271",
            "b8386be4adb34b6c8e09f8757d66531e",
            "1d44c8e7062c4c4fa360c493a13f2d38",
            "b995f407d6c6496a92ba2ac2aa3bb98a",
            "ccbe525a28154342b50e8148f76139f9",
            "e9ba2e64a97d4e99bacddccc8d0b8321",
            "1268fa8667d048a69b16f0d634745938",
            "1f38e134569e436c8c48af4b9f750d0a",
            "de2bbf4d2b824735a4405ecb9eabf4c8",
            "2955f3b69c254a1d83fafd30176cf023",
            "e94a0dbcb58e440ca91572df7001c4b2",
            "8596cf90772642d6abd9997e3ddfb2b0",
            "574a63aab7f944969a3fd5afeee378a2",
            "929ffef8e7174bb7968e84034c4cec6f",
            "fd91e04b99194c29b41e61f01cf248f8",
            "653a9d53bc3c4cc0a51ada34e74eb5eb",
            "b6e0031b7623431eabe32d7ca7802e87",
            "65e8ced102504e4ea6ab4d940c058462",
            "91dfffd978444aed8232f5064d173e1b",
            "69bdbf7341e4497f96298e753683122b",
            "195e79eb86144d65aed6143b2e80b0a3",
            "6eb44b3697f649eabb88b2718630b9ab",
            "6fcf95c143974f6ca8472948c3c4e76f",
            "7721af48a2dc4bba93b0429113416dc1",
            "6f229f10b6834a9fadcd649f3672b6d5",
            "2551162e344b4b489a020ddae6ea82e8",
            "d5467424c937430c842403762170de9f",
            "34e77cdb8fb94af487f79224c336e230",
            "043e7cbbeaa04a928c4e0e05893aa85f",
            "3075ad2798d1431bae0be282f256bb41",
            "f9ab9167388941c8840ec90ad40e6bda",
            "cdbbe414ff434ce1b941e61dfdc798ee",
            "0dbeb4ad35e543a2bfc75bdfc50188c0",
            "45a152a9c4214a39822aa3329e815604",
            "b45d5bad6ca344d9be186fa0a5e4f390",
            "cf3d417c4ef94dc0839d1987b0704b7d",
            "f59ac97008a04a2eaaaa1c5b06c46a98",
            "c95b386793ae44f98b7f740473fed819",
            "2cd6bcee90ab4b89b866f66986214086",
            "26a5c5cdb39047338860934e338cd8e6"
          ]
        },
        "id": "R0pJ6TfpWOrS",
        "outputId": "86af3c0a-94ad-467e-8e20-d190891794d5"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name              | Type              | Params | Mode \n",
            "----------------------------------------------------------------\n",
            "0 | model             | LlamaForCausalLM  | 4.5 B  | eval \n",
            "1 | vision_model      | SiglipVisionModel | 428 M  | eval \n",
            "2 | projection_module | ProjectionModule  | 21.5 M | train\n",
            "3 | loss_fn           | CrossEntropyLoss  | 0      | train\n",
            "----------------------------------------------------------------\n",
            "21.5 M    Trainable params\n",
            "5.0 B     Non-trainable params\n",
            "5.0 B     Total params\n",
            "19,961.320Total estimated model params size (MB)\n",
            "6         Modules in train mode\n",
            "763       Modules in eval mode\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name              | Type              | Params | Mode \n",
            "----------------------------------------------------------------\n",
            "0 | model             | LlamaForCausalLM  | 4.5 B  | eval \n",
            "1 | vision_model      | SiglipVisionModel | 428 M  | eval \n",
            "2 | projection_module | ProjectionModule  | 21.5 M | train\n",
            "3 | loss_fn           | CrossEntropyLoss  | 0      | train\n",
            "----------------------------------------------------------------\n",
            "21.5 M    Trainable params\n",
            "5.0 B     Non-trainable params\n",
            "5.0 B     Total params\n",
            "19,961.320Total estimated model params size (MB)\n",
            "6         Modules in train mode\n",
            "763       Modules in eval mode\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f2256826f274f8fa687ea43a99bb64b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a13f87d131654ce7bf5f12726179aa20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: ? -, -,,???,!, |,.?, - -, -.? -.? -. - - -, -?.? - - -,,, •. -. -,? -, - -, - • - -. - - - -..?.. -? - - •,. -,, -? n\n",
            ". - - - -.\n",
            "    Answer:             a man cooking up dishes at a stall at the night food market in capital          \n",
            " Normed ED: 0.7890625\n",
            "Prediction: “ influential://,.swing.awt-old://ggee.swing influential.log influential.swing respected - — tryingishment able.getElementById://://://://. versa.swing.swing versa celebrated versa versa.ComponentPlacement://.swing.swing.ComponentPlacement ableeenth зрения.ComponentPlacement acclaimed influential importantlyug Arabia — versa importantly versa ableeg.logeg://s influential:// |\n",
            "://.swing accused versa://://.swing:// educated importantlyishment-old://eg hasn versa invited importantly-old acclaimed invited been versa importantly been-old-old explained\n",
            "    Answer:             cute baby elephant walks towards the camera          \n",
            " Normed ED: 0.9023508137432188\n",
            "Prediction: .\n",
            ".\n",
            ".\n",
            ":// /.\n",
            "g.\n",
            "g://.\n",
            ".swing-old://g.swing.swing versa://g acclaimed:// —://..swing://.://.://.\n",
            "ishment.swing зрения president://g president.swing versa versa versa.\n",
            "g.swing://..swing.S.\n",
            "eg.\n",
            ".swing.log.swingeg.\n",
            ".\n",
            " versa acclaimed.swing://s.swing accused conducted://.\n",
            "://.://.\n",
            " acclaimed hundred.swing pleased versa versa influential been able importantly versa://.\n",
            "\n",
            "    Answer:             property image # - beautiful new log cabin nestled in person          \n",
            " Normed ED: 0.8633879781420765\n",
            "Prediction:  .\n",
            " .\n",
            "s\n",
            ".\n",
            " the theers •s .\n",
            "erman –s\n",
            "  .\n",
            "s . .\n",
            "  ;.\n",
            ".\n",
            ". . .\n",
            " .\n",
            " –.  •.  the  the. .\n",
            " the .  ; the the .  • .  .\n",
            "  Pearson the the.  ; .s.  •  the   the    the  the \n",
            "    Answer:             a student gets sleepy during state school .          \n",
            " Normed ED: 0.7607361963190185\n",
            "Prediction: \n",
            "    Answer:             battles for the ball with athlete during the teams final regular - season game of the season .          \n",
            " Normed ED: 1.0\n",
            "Prediction: Â. -person intelligent versa\n",
            " - image.swing.swing.\n",
            ".://\n",
            " versa.swing importantlyperson prominent acclaimed influential importantly..swing:// president.swing://.swing.\n",
            "pe.swing influential educated.swing. the\n",
            ":// able.swing.log..swing trying.swing versa able.\n",
            " importantly able.swing.swing versa respected able versa.swing influential Angelesthe the influential.swing.swing acclaimed acclaimedeg.log.\n",
            " increasingly versa able.swing..swing.swing acclaimed.swing.swing acclaimed importantly.swing.swing acclaimed versa able:// importantly importantly versa://://:// influential evident versa\n",
            "    Answer:             an aerial view during its redevelopment          \n",
            " Normed ED: 0.9081632653061225\n",
            "Prediction: 　 　 　 　\n",
            "    Answer:             the broken old fishing pier on the lake          \n",
            " Normed ED: 0.9508196721311475\n",
            "Prediction: \n",
            "    Answer:             interior of the custom screen door -- extra large size          \n",
            " Normed ED: 1.0\n",
            "Prediction:  |\n",
            " - |\n",
            " |\n",
            "assistant:// |\n",
            " |\n",
            " |\n",
            "me.swing|\n",
            " acclaimed.swing able.\n",
            "â.swing intelligent.ComponentPlacement.log informed versa://ub.log.swing.swing versa versa versa versa influential versa  able://assistant evident acclaimed&nbsp acclaimedassistant:// extensively able.swing |\n",
            "\n",
            " versa importantly.swing grateful been, .swing grateful told://:// \t importantly accused.swingassistant:// accused versa able://&assistant versaishment://\n",
            "    Answer:             view across the beach with its funfair and ferris wheel on a bright sunny day          \n",
            " Normed ED: 0.8574423480083857\n",
            "Prediction: .\n",
            "ess.swing;appro.\n",
            "The. ; ; ;.util.getElementById^://;  ;://initial.swing;.\n",
            "The;://; conducted, Gishment importantlyis.S://,://.\n",
            "-old://.swing importantly.\n",
            ".swing Arabiathe.SThe ://.log versa.log.\n",
            ". the.\n",
            ".\n",
            "  fetischthe influential influentialishment.swing?\n",
            "://://:// appreciated.swing:// appro://.\n",
            "ishment://.\n",
            " extensively://..swing:// versa appreciated versa versa\n",
            "    Answer:             a group of export paintings          \n",
            " Normed ED: 0.9068493150684932\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a44ad2248aee42e0b100ea03d381e8e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction:  a\n",
            " - - - - -? - - -, -??? -? - •? - -? - - - - -? -?? - - - - - - -? - -, - •? -? -? - -? - - - - - - -?? - - -? -? - - - • -? - - -,, - -? - • - - -\n",
            "    Answer:             a man cooking up dishes at a stall at the night food market in capital          \n",
            " Normed ED: 0.76\n",
            "Prediction: .. |, —|.S. — —. - - - - —, - —ates versa:// — - -, - - - - - - |://, —. —.swing hasn:// —.swing:// — influential importantly, - -:// -eg —. —g:// - — -://..log:// -:// —.swing://. acclaimed://.:// — acclaimed.util.swing:// - intelligent versa importantly:// - informed.ComponentPlacement educated\n",
            "    Answer:             cute baby elephant walks towards the camera          \n",
            " Normed ED: 0.8619528619528619\n",
            "Prediction: .,.. —. —.,,,, — -...images.S. - —. —://.eg importantly.logishment versa informed influential importantly.swing versa versa appreciated://, — extensively able - —.eri.S.Segeg —, -er://.. |://,...://. acclaimed:// —the://..log://..Sise.log.swing accused versa.swing://. versa.awt\n",
            "    Answer:             property image # - beautiful new log cabin nestled in person          \n",
            " Normed ED: 0.8345323741007195\n",
            "Prediction: .\n",
            " personers.\n",
            "\n",
            ".\n",
            ".\n",
            "\n",
            "\n",
            ".\n",
            ".\n",
            "erman..\n",
            "s..\n",
            " .ersers..\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            "..\n",
            "ers.\n",
            "..\n",
            "ers. the.ersers..\n",
            ".\n",
            " person legs of of.. ... bread of person.  \\\\ ....\n",
            "person.erman.... \\\\ermanersers. \\\\....s other. Pearson theers.erman.. Pearsoners\n",
            "    Answer:             a student gets sleepy during state school .          \n",
            " Normed ED: 0.8681818181818182\n",
            "Prediction: \n",
            "    Answer:             battles for the ball with athlete during the teams final regular - season game of the season .          \n",
            " Normed ED: 1.0\n",
            "Prediction:  theant,,. the,, a, a, a, an a the the the the the the the. the the the a the acclaimed the, the.swing.\n",
            " the accused a theishment://ology.\n",
            ", the://.. versa.swing the able the.swing versa.swing the-old:// the://, the:// the the grateful closely, the the:// versa.swing.\n",
            ".\n",
            " able extensively versa.swing important:// sons importantly://:// the the://.\n",
            " acclaimed acclaimed:// the://://.swing extensively\n",
            "    Answer:             an aerial view during its redevelopment          \n",
            " Normed ED: 0.875\n",
            "Prediction: \n",
            "    Answer:             the broken old fishing pier on the lake          \n",
            " Normed ED: 1.0\n",
            "Prediction: \n",
            "    Answer:             interior of the custom screen door -- extra large size          \n",
            " Normed ED: 1.0\n",
            "Prediction: , |u | &.swing trying the | versa |the - educated.swingr.amp://mag Arabia. Arabiathe extensively.swing versa informed.. |\n",
            " acclaimedishment.swingteam educated:// versa.swing.swing |f influential acclaimed:// been,  versa | importantly.swing -  |:// -.swing.show, |.swing accused versa able,amp acclaimed versa.swing ://|:// versa.swing versa -amp.log celebrated versa\n",
            "    Answer:             view across the beach with its funfair and ferris wheel on a bright sunny day          \n",
            " Normed ED: 0.8310626702997275\n",
            "Prediction: .initial the,.idea://bes.\n",
            "and,semi.\n",
            ":provide.\n",
            ",, -.\n",
            "the,the,/;.\n",
            ".\n",
            ".***.\n",
            ".\n",
            "initial://on.\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            "the.the.\n",
            ".\n",
            "\"instead.\n",
            ". but,://s.;.idea.the..***.\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            "provide://://://.swing://..\n",
            "://://://the.://://://.swing://on,://://\n",
            "    Answer:             a group of export paintings          \n",
            " Normed ED: 0.9276018099547512\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "345144ccab914e2d8a62279db9912dcb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction:  as\n",
            ",,?,??,, -? a? a -? -,??????????, -????????,, -,????? as?, a? -, the?? -,???? as????? and as??,?,??? and? -.\n",
            "? or -?\n",
            "    Answer:             a man cooking up dishes at a stall at the night food market in capital          \n",
            " Normed ED: 0.8166666666666667\n",
            "Prediction: ,, — -. | the,, — -, the -,.,... the, —.,,, —inperson.swing зрения.log informed.swing amazing influential:// — importantly://.!..://,...://.. -. the.://,. -.,of://,remnonthe versa://, - -the:// -://,the-old acclaimed versa Angeles:// the -:// -\n",
            "    Answer:             cute baby elephant walks towards the camera          \n",
            " Normed ED: 0.8278688524590164\n",
            "Prediction:  the.. the. —, - the an - -, the.. the. |.., —..idour.swing influential.swing able educated://g.util.S://..\n",
            ".://,. | -. -the://, the,.., the the the the the,..the -:// |:// j| the,:// a/ the the |://,://. —:// | versa\n",
            "    Answer:             property image # - beautiful new log cabin nestled in person          \n",
            " Normed ED: 0.8018433179723502\n",
            "Prediction:  person other the.\n",
            "ers.\n",
            " of of.\n",
            " of of of of ofperson..\n",
            "\n",
            " Pearson ofersers Pearson\n",
            ".\n",
            "...\n",
            "..\n",
            ".\n",
            ".\n",
            ".\n",
            " Pearson andpersoners of peopleersers.\n",
            " person.\n",
            " other person and of of Pearson Pearson  Pearsonerman of of person of Pearson Pearson Pearson of Pearson person ofers Pearson ofersers. Pearson of.\n",
            " personerman of of of of Pearson jer person of Pearsonerman of of Pearson\n",
            "    Answer:             a student gets sleepy during state school .          \n",
            " Normed ED: 0.8637602179836512\n",
            "Prediction: \n",
            "    Answer:             battles for the ball with athlete during the teams final regular - season game of the season .          \n",
            " Normed ED: 1.0\n",
            "Prediction:  theegeg a, the the an, the the the, a the.\n",
            " the hundred.\n",
            ", theeg.swing the. the the theegperson the a the.swing.\n",
            " the.swingeg the:// the.swing the the the:// educated.\n",
            " the:// the:// the://.log acclaimed.\n",
            " the://:// the the the:// the the.swing the a the the:// acclaimed.swingeg the versa informed:// extensively extensively able.log.swing.swing:// theeg.\n",
            ".swing influential.swing://eg://.swing:// influential\n",
            "    Answer:             an aerial view during its redevelopment          \n",
            " Normed ED: 0.8832116788321168\n",
            "Prediction: ыџN\n",
            "    Answer:             the broken old fishing pier on the lake          \n",
            " Normed ED: 1.0\n",
            "Prediction:  神马收录\n",
            "    Answer:             interior of the custom screen door -- extra large size          \n",
            " Normed ED: 0.9868421052631579\n",
            "Prediction: ://the | | |-old.swing influential acclaimedthe | |://.swing, the | versa.swing:// versa://&#..swing versa.log. the the.:// informed importantly. evident.log antiqu versa.swing, |person.swing importantly://, |, -. influential://s |.swing, -., | | acclaimed influential:// antiqu.log?. зрения.swing. |\n",
            ".://.swing:// respected influential.swing |\n",
            " versa://\n",
            "    Answer:             view across the beach with its funfair and ferris wheel on a bright sunny day          \n",
            " Normed ED: 0.8305084745762712\n",
            "Prediction: ,the. and,but, and-incorrect,,??-physical- the,-.\n",
            "-idea,,. andpeople,?,right.\n",
            ",://,;butpe://the,liquidright.\n",
            "but.&...\n",
            "because. but, the,right.\n",
            "-and thebest the:// andbutphysical://.Sperson://,.\n",
            ".\n",
            " the://.the..S,://://. and.\n",
            "://\n",
            "    Answer:             a group of export paintings          \n",
            " Normed ED: 0.8986784140969163\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7df540fb8887409c88e4625e6d9039ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: \n",
            " and part and? a\n",
            ", as as a as of? people? a as as? a? a a?????? of of??? and,?? and? as and?? a\n",
            " in as? and the? and and as?? or as an??? and a??? and for a a\n",
            " all as as\n",
            " to as as a a the part as a?\n",
            "\n",
            "    Answer:             a man cooking up dishes at a stall at the night food market in capital          \n",
            " Normed ED: 0.725\n",
            "Prediction:  — — — — -, — the —, — the the the, — — — —the://,, — —end:// -, —um:// —:// —. —:// -. the —://,the:// — the - |:// - |gr? — | the the the - the athe. —. —.util:// the — | the -:// the —:// the the —://, the the the —.swing\n",
            "    Answer:             cute baby elephant walks towards the camera          \n",
            " Normed ED: 0.8258928571428571\n",
            "Prediction:  — — and. - |,. a the the the the the j.,,, —,,., — –li., —., the -Ã://,, -,, of the |., - | the an the a the, - the the the the and the,.,.,.,. the |.\n",
            " —://|. —.\n",
            " the the | | - —:// and\n",
            "    Answer:             property image # - beautiful new log cabin nestled in person          \n",
            " Normed ED: 0.7688172043010753\n",
            "Prediction: \n",
            " person\n",
            " the\n",
            "\n",
            "\n",
            ".\n",
            "\n",
            "...\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            "...\n",
            "...\n",
            "\n",
            " of...\n",
            ".\n",
            "\n",
            " [...th\n",
            ".\n",
            ".\n",
            "\n",
            " Pearson.\n",
            " Pearson...\n",
            ".swingers.\n",
            "\n",
            "201\n",
            " Pearson. 18 and other ...\n",
            "://  Pearson other 201.\n",
            " in the the 1 \\\\ \\\\.\n",
            "......\n",
            ".\n",
            " of Pearson of Pearson\n",
            " Pearson. Pearson of the of and Pearson ofises.\n",
            " and\n",
            "  a  of\n",
            " 39\n",
            "\n",
            "    Answer:             a student gets sleepy during state school .          \n",
            " Normed ED: 0.8529411764705882\n",
            "Prediction: \n",
            "    Answer:             battles for the ball with athlete during the teams final regular - season game of the season .          \n",
            " Normed ED: 1.0\n",
            "Prediction:  aeg- the, a the the, a the an the a, a the the the a a the the the a the the the the the the the the business the:// the the theology versa:// the, the.\n",
            " the:// the the the the the theally the:// the:// the:// the the the the the the the the a the.swinguate versa.\n",
            "eg the:// versa able acclaimed influential://:// conducted..\n",
            " the..log antiqu.\n",
            " the the.\n",
            " the://eg\n",
            "    Answer:             an aerial view during its redevelopment          \n",
            " Normed ED: 0.8818681318681318\n",
            "Prediction: илася\n",
            "    Answer:             the broken old fishing pier on the lake          \n",
            " Normed ED: 1.0\n",
            "Prediction: turnstile: the best of both worlds          \n",
            "    Answer:             interior of the custom screen door -- extra large size          \n",
            " Normed ED: 0.6578947368421053\n",
            "Prediction:  thethe, the the, the:// the | - | the acclaimed versa, | the, the-old., an intelligent versa influential influential.swing, the appreciated.swing hundred versa, the://://://, | -, the.swing,?,amp, -? |,.. influential://,,.://.swing.log://s.,://.swing, | |. closelyishment://s., influential versa\n",
            "    Answer:             view across the beach with its funfair and ferris wheel on a bright sunny day          \n",
            " Normed ED: 0.8074324324324325\n",
            "Prediction: ., the the the and a,- the, the the- a-***- and, the,,,,,,?,,,pe, the the,,.?,,,.\n",
            ".,,, but,,pe, the,,,. the.\n",
            ", the,. the the.\n",
            ", the.\n",
            ". the.\n",
            ".,,..\n",
            " the.\n",
            ". but, the:// the,.app://\n",
            "    Answer:             a group of export paintings          \n",
            " Normed ED: 0.8361581920903954\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac35f286e18f4116b3f097c8605b2b0d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction:  a for as\n",
            " with as in with the the as ofeg b in all is as an as in b\n",
            " a a\n",
            " in as a - of as and as  a as as as in as a as and all in a as the a in the the as a or the as b as a as to b an a as all as of as and as b as to the an as and the as the to 2 to\n",
            "\n",
            "    Answer:             a man cooking up dishes at a stall at the night food market in capital          \n",
            " Normed ED: 0.7509881422924901\n",
            "Prediction:  - — — - -, | | -, — the — | — | - — |, | | — —, | —g -sw://://.swing Angelessp.thethe. thesp.log importantlyperson://-old:// — — |.| the | the the ||sm – -people thethe|://the —..S Angeles://.S theof acclaimed.swing.log://person://ishment Angeles://://inperson:// Arabia\n",
            "    Answer:             cute baby elephant walks towards the camera          \n",
            " Normed ED: 0.8450184501845018\n",
            "Prediction:  — | | | the, the | theerihave the - the, | |in — |, –, | – –.\n",
            " |.\n",
            "||g.\n",
            " -Ã.S.\n",
            " – -eri://eri.log. – – |.\n",
            "eri | the, the the – the – ||.\n",
            " |...\n",
            "eri the the://. the the.://, versa...\n",
            " |!:// the anderi://ishment\n",
            "    Answer:             property image # - beautiful new log cabin nestled in person          \n",
            " Normed ED: 0.8164251207729468\n",
            "Prediction: 0 and the 1 of the Pearson following the following the.\n",
            " following of following and\n",
            " the following...\n",
            "...\n",
            "person other following in for.\n",
            ".\n",
            " Pearson...\n",
            "-old.\n",
            "... the the the our other people.\n",
            "...\n",
            " 1 the the\n",
            " of 11.\n",
            " the the the of the the and...\n",
            "of the the of of the people of the and Pearson in theers...\n",
            "...\n",
            " of and 1...\n",
            "person of :// 11\n",
            "2 1\n",
            "    Answer:             a student gets sleepy during state school .          \n",
            " Normed ED: 0.8625730994152047\n",
            "Prediction: \n",
            "    Answer:             battles for the ball with athlete during the teams final regular - season game of the season .          \n",
            " Normed ED: 1.0\n",
            "Prediction:  the, a, the a, the a, the the the the the, the the the the the the the the the the the the the the the the a the the the; the the. the:// the the the the the the. the the the the the the.S theus educated the.log the the the the the the the a a the:// theous:// the Angeles:// president versa:// the://://:// the the the the.\n",
            " the. the the the:// the://\n",
            "    Answer:             an aerial view during its redevelopment          \n",
            " Normed ED: 0.8810198300283286\n",
            "Prediction: \n",
            "    Answer:             the broken old fishing pier on the lake          \n",
            " Normed ED: 1.0\n",
            "Prediction: \n",
            "    Answer:             interior of the custom screen door -- extra large size          \n",
            " Normed ED: 1.0\n",
            "Prediction: .Sthe, the a the.swing conducted influential versa a a, a. a, a, the influential://, an.swing.swing.log. the, |.ComponentPlacement.swing acclaimedâ influentialishment versa:// versa, |, a.log, a, |, an able,? |...swing,,, |.swing:// versa. |,?:// acclaimed-east.,.log versa://.log, a, accused versa\n",
            "    Answer:             view across the beach with its funfair and ferris wheel on a bright sunny day          \n",
            " Normed ED: 0.7986577181208053\n",
            "Prediction: , the. the the the,- the- the, the, the the,,- it, the, the,,, the, the,, the,, the,, the.\n",
            ", the,.\n",
            ", the,,, the, the.\n",
            " the, the the, the. the, and, the the the...\n",
            ", the the, the, the the. the,.. the the., the the\n",
            "    Answer:             a group of export paintings          \n",
            " Normed ED: 0.8584905660377359\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf18f986903d470db5027076328a4c4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction:  as of and a part as as the is as  the in the and a as with as as or a, as and as as as an in to and as with of as all as as as as as the as on as the as as as as a as as a the is at with as an b as as or as with as\n",
            " a as as with with with as of as as as as or with with as b\n",
            " as\n",
            "    Answer:             a man cooking up dishes at a stall at the night food market in capital          \n",
            " Normed ED: 0.7670250896057348\n",
            "Prediction:  the - to, – |presthe, the the the the | the the - the - — - -, | -, —periodinsp://the.spum the the - and, conducted..| the — the and the thethethe the the the the the andthe, theth thethe thethe the the theof://.log://the the..://the the. acclaimed://.S the the the.S.\n",
            "    Answer:             cute baby elephant walks towards the camera          \n",
            " Normed ED: 0.8327137546468402\n",
            "Prediction:  and the ( in | the - the the the the - the the.. and - - and, by, the - the --.\n",
            " and the |. –. -.the the lastg.\n",
            " ( ( the - the | and an ( the the the |the the the – an the the - the -. andthe.\n",
            "| the the thethe. the the the —..\n",
            ", the the,..\n",
            "\n",
            "    Answer:             property image # - beautiful new log cabin nestled in person          \n",
            " Normed ED: 0.7966804979253111\n",
            "Prediction:  in of\n",
            " the following\n",
            "\n",
            "\n",
            " and of\n",
            " the...\n",
            "...\n",
            ".\n",
            ".\n",
            ".\n",
            "...\n",
            ".\n",
            "...\n",
            "...\n",
            "...\n",
            "...\n",
            " and of the following.\n",
            " of.\n",
            ".\n",
            ".\n",
            " 201 of the\n",
            "\n",
            " of the\n",
            "1\n",
            "\n",
            " and the the 1 and the following the the...\n",
            ".\n",
            " 1.\n",
            " �.\n",
            " of the the...\n",
            " of the\n",
            "...\n",
            "\n",
            "...\n",
            ".\n",
            ".\n",
            "...\n",
            ".\n",
            "\n",
            "\\\\ of\n",
            "0.\n",
            ".\n",
            ".\n",
            "\n",
            ".\n",
            "\n",
            " and\n",
            "\n",
            "    Answer:             a student gets sleepy during state school .          \n",
            " Normed ED: 0.854251012145749\n",
            "Prediction: \n",
            "    Answer:             battles for the ball with athlete during the teams final regular - season game of the season .          \n",
            " Normed ED: 1.0\n",
            "Prediction:  the, the, the the, the the the the the the the the the the the the the the the the the the the the the the the the a the the the the the the the the the the the the the the thethe the the the the the the the the the the the:// the the the the the the the the the the the:// the the? theous trying# the://the.log://ant://, the thethe the the.\n",
            " the the.\n",
            " the.\n",
            "\n",
            "    Answer:             an aerial view during its redevelopment          \n",
            " Normed ED: 0.8997214484679665\n",
            "Prediction: ılmaktadır\n",
            "    Answer:             the broken old fishing pier on the lake          \n",
            " Normed ED: 0.9508196721311475\n",
            "Prediction: ække: window with a custom made screen door - a beautiful example of a functional and beautiful door that is a work of art.          \n",
            "    Answer:             interior of the custom screen door -- extra large size          \n",
            " Normed ED: 0.6992481203007519\n",
            "Prediction:  the, the, a, a the the, an the, a the, a, a..swing, a.:// acclaimed:// the the, the:// able, the... conducted acclaimed importantly.swing, the., the.swing, a, |,.swing, -,,.swing the://,,-east.swing extensively versashow versa, the://://, |\n",
            ",Ã.swing versa, |,,.swing://\n",
            "    Answer:             view across the beach with its funfair and ferris wheel on a bright sunny day          \n",
            " Normed ED: 0.7851851851851852\n",
            "Prediction: , the,,, the,, the, the,.- the,,,,-- a,,,- the,,,, the,, the,,\n",
            ",,,,, the,, the, ( the,, the the, the,, the\n",
            " the,., ( the, the,, the,,,, the, the, the, the,.\n",
            ",,., the\n",
            "    Answer:             a group of export paintings          \n",
            " Normed ED: 0.8363636363636363\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1f89db35ad24514991024b4a6fea069"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction:   a\n",
            " as a little\n",
            " in in as\n",
            " aseg as, but  with as of a and to in of and  with  a, as  a  or  on\n",
            " as with as as as in the in as the as as as as as as as in to a and b a to in fashion b a as  as in  as a  the as as with in as and and as of\n",
            " or a\n",
            "    Answer:             a man cooking up dishes at a stall at the night food market in capital          \n",
            " Normed ED: 0.7366255144032922\n",
            "Prediction: ,,, the,,| —, the, the the -, the, |, - and — | — -,,person,thein hundredthethein://the the the.theintÃ.non://the - the,thefilm; the the the the thethe the theintthethe theintthe theleg://://://. thein:// Angeles.\n",
            "the thethethe the.spouslin://://\n",
            "    Answer:             cute baby elephant walks towards the camera          \n",
            " Normed ED: 0.8414634146341463\n",
            "Prediction:  in the | | | the the the the the the the the. the.\n",
            ", – | a|  as - and – fromg. - Ã/ the the | the. -, the - - | - the, | – and the an, the the the the the the and, last, | the - and the..\n",
            " the the.the|...\n",
            " the. the..\n",
            " the the|.\n",
            "\n",
            "    Answer:             property image # - beautiful new log cabin nestled in person          \n",
            " Normed ED: 0.7991266375545851\n",
            "Prediction:  of\n",
            " of\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            ".\n",
            "\n",
            "\n",
            "\n",
            "\n",
            ".\n",
            "...\n",
            ".\n",
            "...\n",
            "\n",
            "\n",
            "\n",
            " the.\n",
            "\n",
            " of.\n",
            "\n",
            ".\n",
            " \n",
            " and.\n",
            "\n",
            " and\n",
            " of \n",
            "\n",
            "\n",
            "\n",
            " and\n",
            "\n",
            " the \n",
            "\n",
            "\n",
            "\n",
            "  a  \n",
            " of of \n",
            "\n",
            ".\n",
            ".\n",
            ".\n",
            " |\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    Answer:             a student gets sleepy during state school .          \n",
            " Normed ED: 0.8518518518518519\n",
            "Prediction: .Guna: person celebrates after scoring during the match.          \n",
            "    Answer:             battles for the ball with athlete during the teams final regular - season game of the season .          \n",
            " Normed ED: 0.6896551724137931\n",
            "Prediction:  the, the, the, the the, the, the, the, the the the the, the the the, the, the the the the the the the the the thethe the the the the thethe the the the the the the the the the the theassist the the thethe thethe thethe the the the the the the the the thethe the the thethethe; versa:// the‎.swingthe the the thethetheththe;the thethe thethe\n",
            "    Answer:             an aerial view during its redevelopment          \n",
            " Normed ED: 0.8944281524926686\n",
            "Prediction: \n",
            "    Answer:             the broken old fishing pier on the lake          \n",
            " Normed ED: 1.0\n",
            "Prediction: ıldığı\n",
            "    Answer:             interior of the custom screen door -- extra large size          \n",
            " Normed ED: 0.9868421052631579\n",
            "Prediction: .,, a a,,,, a, a,,,, |, |, an, |, |Ã importantly the, the,er versatypethe....,, |,,,,, |,, theous able –,,... |,://assistant.swing.swing the., |\n",
            ".swing. |\n",
            ",,er:// importantly the.,,://.swing\n",
            "    Answer:             view across the beach with its funfair and ferris wheel on a bright sunny day          \n",
            " Normed ED: 0.7894736842105263\n",
            "Prediction:  the,-- the,, the.,,-,., the,,,,-., the,,,,,, the,,, the, the., the,,,., but,,,, the., the,.,\n",
            "\n",
            ",\n",
            ",..,\n",
            ",,,.,,,,,,.\n",
            ", the.,..- the,. a,\n",
            "    Answer:             a group of export paintings          \n",
            " Normed ED: 0.8796992481203008\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f38e134569e436c8c48af4b9f750d0a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction:  as bent a in a with a as on a of as a a a weekend as at at with foreign but\n",
            " as as a more and a to  in as is b a\n",
            " with a as\n",
            " a\n",
            " on as to in as as on a little as a  in at to as to is as the a person in the a in a to a with a an but as a as and as as at to as in as\n",
            "    Answer:             a man cooking up dishes at a stall at the night food market in capital          \n",
            " Normed ED: 0.7424242424242424\n",
            "Prediction:  the the the the a the the the the the the the the the the the the the | the, the. the in, and | the and the and, thethe thethe the the the thethe theperson. the — the the thethe the the the the the the the the the the the the the the the thethe the the theperson| the the the theperson the the the thethethe the the the,the\n",
            "    Answer:             cute baby elephant walks towards the camera          \n",
            " Normed ED: 0.8641975308641975\n",
            "Prediction:  and the, the, the the the the the the the the the in the|       —        for -- last — last —s - last in the an the - the,the | j —        - and ( the the – the — the the, the the the the the the - the,, the the the last the the the the the the | the it the the and, the – the       —,\n",
            "    Answer:             property image # - beautiful new log cabin nestled in person          \n",
            " Normed ED: 0.8292682926829268\n",
            "Prediction:  the the and the\n",
            " the\n",
            " for\n",
            " and\n",
            " and\n",
            "201\\\\.\n",
            " the and\n",
            " the 0\n",
            " the\n",
            " the person in the the person.\n",
            " the, the the\n",
            "20 and\n",
            " and the\n",
            " 202.\n",
            ": and:2.\n",
            " the\n",
            " 201.\n",
            " 20 201|...\n",
            ".\n",
            " the for the: and and\n",
            " and 201\n",
            " of and\n",
            "âand and and\n",
            "1.\n",
            "\n",
            " and\n",
            "âlasts\n",
            "    Answer:             a student gets sleepy during state school .          \n",
            " Normed ED: 0.8154506437768241\n",
            "Prediction: ække: person vies for the ball during the match between and.          \n",
            "    Answer:             battles for the ball with athlete during the teams final regular - season game of the season .          \n",
            " Normed ED: 0.5948275862068966\n",
            "Prediction:  the, the the the, the the, the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the of the the the the the the the the the the the the the the the the the the the the the the the the the the the thethe the the the the the the the the the the the the the the the the the\n",
            "    Answer:             an aerial view during its redevelopment          \n",
            " Normed ED: 0.905511811023622\n",
            "Prediction: \n",
            "    Answer:             the broken old fishing pier on the lake          \n",
            " Normed ED: 1.0\n",
            "Prediction: еристи\n",
            "    Answer:             interior of the custom screen door -- extra large size          \n",
            " Normed ED: 1.0\n",
            "Prediction:  the, the the, a, the,, a,,,, a, a,,, the, the.swing..the a, thethous.swing, a the thethe the, |, the.log, a,,, the.,,, |,,, |, the theâ the&#, | | the.,; the |\n",
            " respected.,, a, the://\n",
            "    Answer:             view across the beach with its funfair and ferris wheel on a bright sunny day          \n",
            " Normed ED: 0.7663043478260869\n",
            "Prediction: ,, the, the, the but,, the, the, (,- the, the but the,, the,-,, a,,, the, the,, the the,,,,,.\n",
            ",.\n",
            ", a,.\n",
            " the, the.\n",
            ",,.\n",
            ", the h the but,,, the..\n",
            ",,.\n",
            " the,,,, the, the the, the but,. the (\n",
            "    Answer:             a group of export paintings          \n",
            " Normed ED: 0.8378378378378378\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91dfffd978444aed8232f5064d173e1b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction:  the\n",
            " as a\n",
            " with as in as in as weekend of  and as in b more as with to  to  at with as more in of with as b a breakdown with\n",
            " the as  the a with but at at as as  as or an as as with as little as a powers in as b a an the for\n",
            " as more in the c as more as as as a as as and with as or or all\n",
            "    Answer:             a man cooking up dishes at a stall at the night food market in capital          \n",
            " Normed ED: 0.7655172413793103\n",
            "Prediction:  — the, the the |insthe the the the the the the the the - the the |sex —— — the forthethe thepres influential thebusiness://thethe thethe.leg://person.swing. -the the the the theremthe the | the the the thepig - thetheins. the. thethis!the!the the theth versa the thethe the.://it://thethe the the the\n",
            "    Answer:             cute baby elephant walks towards the camera          \n",
            " Normed ED: 0.8504983388704319\n",
            "Prediction:  as as in in the - the the the the the - - - - - —. - -    - and -- — — the - --spthe the – –the.\n",
            " - | - - the - | | — – as in | the the last, the - the | the - |. - the the the the -.\n",
            "the the the, thethethe  thethe...the, - the the\n",
            "    Answer:             property image # - beautiful new log cabin nestled in person          \n",
            " Normed ED: 0.8017241379310345\n",
            "Prediction:   of the\n",
            "\n",
            "\n",
            " the the\n",
            " the\n",
            "\n",
            " the and\n",
            ", 2\n",
            "...\n",
            "...\n",
            " and a the and\n",
            "201 a\n",
            " person.\n",
            " following the the 1\n",
            "202\n",
            "\n",
            "...\n",
            " \n",
            "\n",
            " of the\n",
            "\n",
            "\n",
            " the 0 and following the 202\n",
            " and 202\n",
            " Pearson\n",
            "\n",
            "â 2 the...\n",
            " 0.\n",
            "\n",
            " and\n",
            " of\n",
            " of\n",
            " 2 16\n",
            "\n",
            " and\n",
            " 202 and first 202\n",
            "\n",
            "    Answer:             a student gets sleepy during state school .          \n",
            " Normed ED: 0.8026315789473685\n",
            "Prediction: \n",
            "    Answer:             battles for the ball with athlete during the teams final regular - season game of the season .          \n",
            " Normed ED: 1.0\n",
            "Prediction:  the, the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the:// the the the the the the000 the the the the the the the the the the the the the the the the the the the the thethe thethe the the the the the the.S the:// the the the the the.swing://.util:// the the the the:// the:// the the the the.S the\n",
            "    Answer:             an aerial view during its redevelopment          \n",
            " Normed ED: 0.9050131926121372\n",
            "Prediction: ávajícíávacíturnstile - a broken bridge in a lake          ЎыџN\n",
            "    Answer:             the broken old fishing pier on the lake          \n",
            " Normed ED: 0.6666666666666666\n",
            "Prediction: ıldığındaılmaktadırávací\n",
            "    Answer:             interior of the custom screen door -- extra large size          \n",
            " Normed ED: 0.9342105263157895\n",
            "Prediction: ., the,, | the the the, the, the thethe | a the,, | the..://ishment.swing., a | the acclaimedishment versa the:// been theishment the, a the.swing the the,, a | the, - |. the.swing the |\n",
            ",:// the.swing influential influential., the://&nbsp versa,, the.log grateful held the. the://.swing\n",
            "    Answer:             view across the beach with its funfair and ferris wheel on a bright sunny day          \n",
            " Normed ED: 0.7979094076655052\n",
            "Prediction: ,\n",
            "\n",
            ",.\n",
            "\n",
            " the the and the..\n",
            "\n",
            "., the the,,,,-, the,,,., the the\n",
            ",,, the, the, a., the.\n",
            ",,.,. in but the..,.\n",
            " but.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " by and a\n",
            ",,\n",
            ". a,,,,.,. the, the., the.\n",
            "..\n",
            " the\n",
            "    Answer:             a group of export paintings          \n",
            " Normed ED: 0.8395061728395061\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3075ad2798d1431bae0be282f256bb41"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: \n",
            " F\n",
            " an but as or a\n",
            " with aepent on but at by as with 191 b as is  ```\n",
            "  or extra 198 in more as a as as\n",
            "   more as\n",
            " from at b as as as  or a to as a to forum with b a but foreign at as with as as a 3 as as with as all as as on to as fashion as or as as\n",
            " in \n",
            "    Answer:             a man cooking up dishes at a stall at the night food market in capital          \n",
            " Normed ED: 0.751937984496124\n",
            "Prediction: , the the, the it an, the the the the the theins the the the, but |person - - - | the the thetheremaaaaâthe the thethethethe thethe thethe the the the the the theremthe | the the the the thethe the thethe thethethethe,allyÃ thethe thethe thethethe the the thethe the thethe the thethe the thethe\n",
            "    Answer:             cute baby elephant walks towards the camera          \n",
            " Normed ED: 0.8542372881355932\n",
            "Prediction: , it, - in, the the the the the the the, - --.\n",
            " | | - – –, - the - - fromlinthe. the the the.inÃthe, | the - the - - ( |. | – the an the the the the the the - – | the.the thetheThe the the the thethethethe thethe the theThe the.the,.jpg://\n",
            "    Answer:             property image # - beautiful new log cabin nestled in person          \n",
            " Normed ED: 0.8158995815899581\n",
            "Prediction: \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " the, is\n",
            "\n",
            "\n",
            " the the\n",
            " the the the person 202 201\n",
            "\n",
            "201\n",
            " and\n",
            " the person\n",
            "201\n",
            "\n",
            " 1\n",
            "0 the 0\n",
            " 201\n",
            " 201\n",
            "\n",
            " people, 0\n",
            "201\n",
            "10\n",
            " is\n",
            "202\n",
            " 2\n",
            " 202\n",
            " and\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "202\n",
            "\n",
            "    Answer:             a student gets sleepy during state school .          \n",
            " Normed ED: 0.8282208588957055\n",
            "Prediction: \n",
            "    Answer:             battles for the ball with athlete during the teams final regular - season game of the season .          \n",
            " Normed ED: 1.0\n",
            "Prediction:  the the, the, the the, the the the the the the the the the the the the the the the the the the the the the the the the the the theant the a the the the theÃ thethe the the the the the the the the the the:// the the:// the the the the the, the the the the the the versa:// theishment influential.swing versa:// versa:// hundred:// able:// the the thethe.\n",
            " the theating the the the theating\n",
            "    Answer:             an aerial view during its redevelopment          \n",
            " Normed ED: 0.8894601542416453\n",
            "Prediction: ıldığındaыџN\n",
            "    Answer:             the broken old fishing pier on the lake          \n",
            " Normed ED: 0.9344262295081968\n",
            "Prediction: \n",
            "    Answer:             interior of the custom screen door -- extra large size          \n",
            " Normed ED: 1.0\n",
            "Prediction: ,, the,,,, |person.log, the,,,,, |,,,, |..log informed:// the |\n",
            "the theÃ evident influential the the://.swing.swingishment the, | the.swing the., |, theous://,,.; |\n",
            ".,org | educated informed.swing able, | the://? theus |\n",
            " conducted versa|assistant://,the prominent informed\n",
            "    Answer:             view across the beach with its funfair and ferris wheel on a bright sunny day          \n",
            " Normed ED: 0.8168498168498168\n",
            "Prediction:  a\n",
            " the,\n",
            ",,. the.\n",
            " the,..\n",
            ", the-.\n",
            "\n",
            ".\n",
            " but\n",
            ".\n",
            ", but, the the,\n",
            ",\n",
            ".\n",
            "\n",
            " a,.,\n",
            ".   is., the..\n",
            " a, the...\n",
            "\n",
            ".\n",
            "\n",
            "\n",
            "\n",
            " but the, a, but in the a the.\n",
            ", a\n",
            ", the\n",
            ".\n",
            " the. the a. the\n",
            " (\n",
            "    Answer:             a group of export paintings          \n",
            " Normed ED: 0.8121212121212121\n"
          ]
        }
      ],
      "source": [
        "trainer.fit(model_module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT-no1yCN_7R"
      },
      "outputs": [],
      "source": [
        "torch.save(model_module.projection_module.state_dict(), \"/content/drive/MyDrive/model/llama3_multi.bin\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArCPEJjLGpbf"
      },
      "source": [
        "## metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F43S996OGrRN"
      },
      "outputs": [],
      "source": [
        "# model_module.load_state_dict(torch.load(\"/content/drive/MyDrive/model/llama3_multi.bin\"), strict=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5a1pQFrEC9m"
      },
      "outputs": [],
      "source": [
        "model_module.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32TZGW9Q5bMJ"
      },
      "outputs": [],
      "source": [
        "model_module.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8m4u1aLGrTS"
      },
      "outputs": [],
      "source": [
        "dd = DataLoader(train_dataset, collate_fn=data_collator, batch_size=2, shuffle=True, num_workers=0)\n",
        "batch = next(iter(dd))\n",
        "\n",
        "# batch = train_dataset[0]\n",
        "input_ids = batch['input_ids'].to(device=model_module.device) # value long\n",
        "pixel_values = batch['pixel_values'].to(device=model_module.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyWeTdnn9CQu"
      },
      "outputs": [],
      "source": [
        "attention_mask = batch['attention_mask']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sAa_uYbmT6E"
      },
      "outputs": [],
      "source": [
        "pixel_values.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkrfvbLpGrVS"
      },
      "outputs": [],
      "source": [
        "\n",
        "# with autocast():\n",
        "image_forward_outs = model_module.vision_model(\n",
        "    pixel_values.to(device=model_module.device), #.unsqueeze(0),\n",
        "    output_hidden_states=True,\n",
        ") # value float16\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hm4jXGE7n3zX"
      },
      "outputs": [],
      "source": [
        "image_forward_outs.last_hidden_state.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EByyX018n32A"
      },
      "outputs": [],
      "source": [
        "image_features = image_forward_outs.hidden_states[-2]\n",
        "projected_embeddings = model_module.projection_module(image_features).to(model_module.device) # module float32 + value float16\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "427B2SWyoISU"
      },
      "outputs": [],
      "source": [
        "projected_embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yftpc8OWzFqA"
      },
      "outputs": [],
      "source": [
        "embedding_layer = model_module.model.get_input_embeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAVvDF7uzpBD"
      },
      "outputs": [],
      "source": [
        "input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGU8HQRUI-fo"
      },
      "outputs": [],
      "source": [
        "total_ids = []\n",
        "total_attn = []\n",
        "\n",
        "for i in range(input_ids.shape[0]):\n",
        "  print(i)\n",
        "  input = input_ids[i].unsqueeze(0)\n",
        "  attention = attention_mask[i].unsqueeze(0)\n",
        "  image = projected_embeddings[i].unsqueeze(0)\n",
        "\n",
        "  if not isinstance(input, torch.Tensor):\n",
        "      input = torch.tensor(input)\n",
        "\n",
        "  split_index = (input == 500000).nonzero(as_tuple=True)[1]\n",
        "\n",
        "  input_ids_1 = input[:, :split_index]\n",
        "  input_ids_2 = input[:, split_index + 1 :]\n",
        "\n",
        "  # Convert input_ids to embeddings\n",
        "  embeddings_1 = embedding_layer(input_ids_1)\n",
        "  embeddings_2 = embedding_layer(input_ids_2)\n",
        "\n",
        "  device = image.device\n",
        "  token_embeddings_part1 = embeddings_1.to(device)\n",
        "  token_embeddings_part2 = embeddings_2.to(device)\n",
        "\n",
        "  # Concatenate the token embeddings and image features\n",
        "  concatenated_embedding = torch.cat(\n",
        "      [token_embeddings_part1, image, token_embeddings_part2], dim=1\n",
        "  )\n",
        "\n",
        "  attention_mask_1 = attention[:, :split_index]\n",
        "  attention_mask_2 = attention[:, split_index + 1 :]\n",
        "  image = torch.ones(\n",
        "      image.shape[:2], dtype=torch.long\n",
        "  )\n",
        "  idevice = image.device\n",
        "  cat_attention_mask = torch.cat(\n",
        "      [attention_mask_1.to(idevice), image, attention_mask_2.to(idevice)], dim=1\n",
        "  )\n",
        "  print(cat_attention_mask)\n",
        "  # # Create the corrected attention mask\n",
        "  # attention_mask = torch.ones(\n",
        "  #     concatenated_embedding.shape[:2], dtype=torch.long\n",
        "  # )\n",
        "\n",
        "  total_ids.append(concatenated_embedding)\n",
        "\n",
        "  total_attn.append(cat_attention_mask)\n",
        "\n",
        "concatenated_embeddings = torch.cat(total_ids, dim=0)\n",
        "cat_attention_masks = torch.cat(total_attn, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4YhgcriI-iH"
      },
      "outputs": [],
      "source": [
        "cat_attention_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iLvCln4zDVo"
      },
      "outputs": [],
      "source": [
        "split_index = (input_ids == 500000).nonzero(as_tuple=True)[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6HqCW6e9ZH7"
      },
      "outputs": [],
      "source": [
        "(input_ids == 500000).nonzero(as_tuple=True)[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJh1T0Hn8Pwu"
      },
      "outputs": [],
      "source": [
        "input_ids_1 = input_ids[:, :split_index]\n",
        "input_ids_2 = input_ids[:, split_index + 1 :]\n",
        "\n",
        "# Convert input_ids to embeddings\n",
        "embeddings_1 = embedding_layer(input_ids_1)\n",
        "embeddings_2 = embedding_layer(input_ids_2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKdcdbDe06IB"
      },
      "outputs": [],
      "source": [
        "device = image_features.device\n",
        "token_embeddings_part1 = embeddings_1.to(device)\n",
        "token_embeddings_part2 = embeddings_2.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6n0ajKK008TH"
      },
      "outputs": [],
      "source": [
        "# Concatenate the token embeddings and image features\n",
        "concatenated_embeddings = torch.cat(\n",
        "    [token_embeddings_part1, projected_embeddings, token_embeddings_part2], dim=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snS-mqMD1hVl"
      },
      "outputs": [],
      "source": [
        "# Create the corrected attention mask\n",
        "attention_mask = torch.ones(\n",
        "    concatenated_embeddings.shape[:2], dtype=torch.long\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mMvYX7eoGCs"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# 배치 데이터 가능\n",
        "new_embeds,attn_mask = process_tensors(\n",
        "    input_ids, projected_embeddings, embedding_layer\n",
        ") # value float16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdstkxnR1wS6"
      },
      "outputs": [],
      "source": [
        "attn_mask.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljnT-JZ62AlU"
      },
      "outputs": [],
      "source": [
        "split_index = (input_ids == 500000).nonzero(as_tuple=True)[1][0]\n",
        "\n",
        "input_ids_1 = input_ids[:, :split_index]\n",
        "input_ids_2 = input_ids[:, split_index + 1 :]\n",
        "\n",
        "device = image_features.device\n",
        "pbatch = image_features.shape[0]\n",
        "pseq = image_features.shape[1]\n",
        "image_token = torch.full([pbatch,pseq,], -100, dtype=torch.long).to(model_module.device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zo5xRz7Z2R0u"
      },
      "outputs": [],
      "source": [
        "input_ids_2.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAD7t-_E2J6k"
      },
      "outputs": [],
      "source": [
        "# Concatenate the token embeddings and image features\n",
        "concatenated_embeddings = torch.cat(\n",
        "    [input_ids_1, image_token, input_ids_2], dim=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkHZC4-iF1c5"
      },
      "outputs": [],
      "source": [
        "labels = process_labels(\n",
        "    input_ids, projected_embeddings\n",
        ") # value float16\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aHn5CIW1ufX"
      },
      "outputs": [],
      "source": [
        "attn_mask = attn_mask.to(model_module.device)\n",
        "new_embeds = new_embeds.to(model_module.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OF3RrjBmvZ4o"
      },
      "outputs": [],
      "source": [
        "\n",
        "labels = labels.to(model_module.device)\n",
        "\n",
        "outputs = model_module.model(inputs_embeds=new_embeds, attention_mask=attn_mask) # module float32 + value float16\n",
        "logits = outputs.logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkANyNIivZ7o"
      },
      "outputs": [],
      "source": [
        "dd = DataLoader(val_dataset, collate_fn=data_collator2, batch_size=2, shuffle=True, num_workers=0)\n",
        "batch = next(iter(dd))\n",
        "device = model_module.device\n",
        "\n",
        "\n",
        "input_ids = batch['input_ids'].to(device) # value long\n",
        "pixel_values = batch['pixel_values'].to(device)\n",
        "attention_mask = batch['attention_mask']\n",
        "answers = batch['answers']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EH68oEPP6cE7"
      },
      "outputs": [],
      "source": [
        "# with autocast():\n",
        "image_forward_outs = model_module.vision_model(\n",
        "    pixel_values.to(device=device), #.unsqueeze(0),\n",
        "    output_hidden_states=True,\n",
        ") # value float16\n",
        "\n",
        "image_features = image_forward_outs.hidden_states[-2]\n",
        "projected_embeddings = model_module.projection_module(image_features).to(device) # module float32 + value float16\n",
        "\n",
        "embedding_layer = model_module.model.get_input_embeddings()\n",
        "\n",
        "# 배치 데이터 가능\n",
        "new_embeds, attn_mask = process_tensors(\n",
        "    input_ids, attention_mask, projected_embeddings, embedding_layer\n",
        ") # value float16\n",
        "\n",
        "attn_mask = attn_mask.to(device)\n",
        "new_embeds = new_embeds.to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jg21Qrbd7uS7"
      },
      "outputs": [],
      "source": [
        "attn_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-N4RGNQT7xoP"
      },
      "outputs": [],
      "source": [
        "new_embeds.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TapBApdfvZ-P"
      },
      "outputs": [],
      "source": [
        "# autoregressively generate token IDs\n",
        "generated_ids = model_module.model.generate(inputs_embeds=new_embeds.to(dtype=torch.float16), attention_mask=attn_mask, max_new_tokens=128)\n",
        "# turn them back into text, chopping of the prompt\n",
        "# important: we don't skip special tokens here, because we want to see them in the output\n",
        "predictions = tokenizer.batch_decode(generated_ids[:, input_ids.size(1):], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nB8R2mJgH1jy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFCy3SYc7l_x"
      },
      "outputs": [],
      "source": [
        "text = \"What is transformer attention?\"\n",
        "input= tokenizer([text], return_tensors=\"pt\")\n",
        "embed = embedding_layer(input['input_ids'].to(model_module.device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hfjgTXU7mCb"
      },
      "outputs": [],
      "source": [
        "generate_ids  = model_module.model.generate(inputs_embeds = embed.to(dtype=torch.float16) , attention_mask = input['attention_mask'], max_new_tokens=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HgWidno7mFG"
      },
      "outputs": [],
      "source": [
        "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "T0J5lbxNjHWO",
        "CqcpNc2S9nnY",
        "Lb_43UOH4DGr",
        "SYlCeoZPlvLI",
        "ArCPEJjLGpbf"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "111c0414d7e44060ad69bf50e1139109": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "3549fb9401e54c59a51cd7f7c37ed7df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4b9bde8de9814c46a50d4ea01c5e3b9a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6066e4c201bd4c14ae708a4d1aa03d37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b350cce1d4cf420fa9d690ce4841b2b6",
            "placeholder": "​",
            "style": "IPY_MODEL_c1143e07384547e2b0d85d3a28a99a40",
            "value": " 10/10 [02:45&lt;00:00,  0.06it/s]"
          }
        },
        "60bf3b0eb2dc4fb08dc6a95571ee488d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99133bade4ed479797e17a63c62aede6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b9bde8de9814c46a50d4ea01c5e3b9a",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3549fb9401e54c59a51cd7f7c37ed7df",
            "value": 10
          }
        },
        "a30f96d101aa4f55b2b5df0923638912": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3508455d7564a4fabf31d8f62299b8d",
            "placeholder": "​",
            "style": "IPY_MODEL_60bf3b0eb2dc4fb08dc6a95571ee488d",
            "value": "Validation DataLoader 0: 100%"
          }
        },
        "b350cce1d4cf420fa9d690ce4841b2b6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1143e07384547e2b0d85d3a28a99a40": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf18f986903d470db5027076328a4c4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a30f96d101aa4f55b2b5df0923638912",
              "IPY_MODEL_99133bade4ed479797e17a63c62aede6",
              "IPY_MODEL_6066e4c201bd4c14ae708a4d1aa03d37"
            ],
            "layout": "IPY_MODEL_111c0414d7e44060ad69bf50e1139109"
          }
        },
        "f3508455d7564a4fabf31d8f62299b8d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1f89db35ad24514991024b4a6fea069": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b34536a60f404b6d9b5106ef3b69ce6d",
              "IPY_MODEL_858824b05c6e47bbbe0209f1dc7ab434",
              "IPY_MODEL_813fc61e4ccc45509f64276dcc023e91"
            ],
            "layout": "IPY_MODEL_78270360941740f987c942f3139d5271"
          }
        },
        "b34536a60f404b6d9b5106ef3b69ce6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8386be4adb34b6c8e09f8757d66531e",
            "placeholder": "​",
            "style": "IPY_MODEL_1d44c8e7062c4c4fa360c493a13f2d38",
            "value": "Validation DataLoader 0: 100%"
          }
        },
        "858824b05c6e47bbbe0209f1dc7ab434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b995f407d6c6496a92ba2ac2aa3bb98a",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ccbe525a28154342b50e8148f76139f9",
            "value": 10
          }
        },
        "813fc61e4ccc45509f64276dcc023e91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9ba2e64a97d4e99bacddccc8d0b8321",
            "placeholder": "​",
            "style": "IPY_MODEL_1268fa8667d048a69b16f0d634745938",
            "value": " 10/10 [02:45&lt;00:00,  0.06it/s]"
          }
        },
        "78270360941740f987c942f3139d5271": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "b8386be4adb34b6c8e09f8757d66531e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d44c8e7062c4c4fa360c493a13f2d38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b995f407d6c6496a92ba2ac2aa3bb98a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccbe525a28154342b50e8148f76139f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e9ba2e64a97d4e99bacddccc8d0b8321": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1268fa8667d048a69b16f0d634745938": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f38e134569e436c8c48af4b9f750d0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de2bbf4d2b824735a4405ecb9eabf4c8",
              "IPY_MODEL_2955f3b69c254a1d83fafd30176cf023",
              "IPY_MODEL_e94a0dbcb58e440ca91572df7001c4b2"
            ],
            "layout": "IPY_MODEL_8596cf90772642d6abd9997e3ddfb2b0"
          }
        },
        "de2bbf4d2b824735a4405ecb9eabf4c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_574a63aab7f944969a3fd5afeee378a2",
            "placeholder": "​",
            "style": "IPY_MODEL_929ffef8e7174bb7968e84034c4cec6f",
            "value": "Validation DataLoader 0: 100%"
          }
        },
        "2955f3b69c254a1d83fafd30176cf023": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd91e04b99194c29b41e61f01cf248f8",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_653a9d53bc3c4cc0a51ada34e74eb5eb",
            "value": 10
          }
        },
        "e94a0dbcb58e440ca91572df7001c4b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6e0031b7623431eabe32d7ca7802e87",
            "placeholder": "​",
            "style": "IPY_MODEL_65e8ced102504e4ea6ab4d940c058462",
            "value": " 10/10 [02:45&lt;00:00,  0.06it/s]"
          }
        },
        "8596cf90772642d6abd9997e3ddfb2b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "574a63aab7f944969a3fd5afeee378a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "929ffef8e7174bb7968e84034c4cec6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd91e04b99194c29b41e61f01cf248f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "653a9d53bc3c4cc0a51ada34e74eb5eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6e0031b7623431eabe32d7ca7802e87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65e8ced102504e4ea6ab4d940c058462": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91dfffd978444aed8232f5064d173e1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_69bdbf7341e4497f96298e753683122b",
              "IPY_MODEL_195e79eb86144d65aed6143b2e80b0a3",
              "IPY_MODEL_6eb44b3697f649eabb88b2718630b9ab"
            ],
            "layout": "IPY_MODEL_6fcf95c143974f6ca8472948c3c4e76f"
          }
        },
        "69bdbf7341e4497f96298e753683122b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7721af48a2dc4bba93b0429113416dc1",
            "placeholder": "​",
            "style": "IPY_MODEL_6f229f10b6834a9fadcd649f3672b6d5",
            "value": "Validation DataLoader 0: 100%"
          }
        },
        "195e79eb86144d65aed6143b2e80b0a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2551162e344b4b489a020ddae6ea82e8",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d5467424c937430c842403762170de9f",
            "value": 10
          }
        },
        "6eb44b3697f649eabb88b2718630b9ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34e77cdb8fb94af487f79224c336e230",
            "placeholder": "​",
            "style": "IPY_MODEL_043e7cbbeaa04a928c4e0e05893aa85f",
            "value": " 10/10 [02:45&lt;00:00,  0.06it/s]"
          }
        },
        "6fcf95c143974f6ca8472948c3c4e76f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "7721af48a2dc4bba93b0429113416dc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f229f10b6834a9fadcd649f3672b6d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2551162e344b4b489a020ddae6ea82e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5467424c937430c842403762170de9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "34e77cdb8fb94af487f79224c336e230": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "043e7cbbeaa04a928c4e0e05893aa85f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3075ad2798d1431bae0be282f256bb41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9ab9167388941c8840ec90ad40e6bda",
              "IPY_MODEL_cdbbe414ff434ce1b941e61dfdc798ee",
              "IPY_MODEL_0dbeb4ad35e543a2bfc75bdfc50188c0"
            ],
            "layout": "IPY_MODEL_45a152a9c4214a39822aa3329e815604"
          }
        },
        "f9ab9167388941c8840ec90ad40e6bda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b45d5bad6ca344d9be186fa0a5e4f390",
            "placeholder": "​",
            "style": "IPY_MODEL_cf3d417c4ef94dc0839d1987b0704b7d",
            "value": "Validation DataLoader 0: 100%"
          }
        },
        "cdbbe414ff434ce1b941e61dfdc798ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f59ac97008a04a2eaaaa1c5b06c46a98",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c95b386793ae44f98b7f740473fed819",
            "value": 10
          }
        },
        "0dbeb4ad35e543a2bfc75bdfc50188c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cd6bcee90ab4b89b866f66986214086",
            "placeholder": "​",
            "style": "IPY_MODEL_26a5c5cdb39047338860934e338cd8e6",
            "value": " 10/10 [02:45&lt;00:00,  0.06it/s]"
          }
        },
        "45a152a9c4214a39822aa3329e815604": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "b45d5bad6ca344d9be186fa0a5e4f390": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf3d417c4ef94dc0839d1987b0704b7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f59ac97008a04a2eaaaa1c5b06c46a98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c95b386793ae44f98b7f740473fed819": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2cd6bcee90ab4b89b866f66986214086": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26a5c5cdb39047338860934e338cd8e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}